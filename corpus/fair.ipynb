{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751935419146
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL_NAME = \"facebook/perturber\"\n",
    "SEP_TOKEN  = \"<PERT_SEP>\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751935424796
    }
   },
   "outputs": [],
   "source": [
    "#test\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "def _simplify(text: str) -> str:\n",
    "    return re.sub(rf\"[{re.escape(string.punctuation)}\\s]+\", \"\", text.lower())\n",
    "\n",
    "SUPPORTED: Dict[str, Set[str]] = {\n",
    "    \"gender\": {\"man\", \"woman\", \"non-binary\"},\n",
    "    \"race\": {\"black\", \"white\", \"asian\", \"hispanic\", \"native-american\", \"pacific-islander\"},\n",
    "}\n",
    "\n",
    "def norm(w: str) -> str:\n",
    "    return re.sub(r\"[^\\w'-]+\", \"\", w.lower())\n",
    "\n",
    "PRIORITY_WORDS: Dict[str, Set[str]] = {\n",
    "    \"gender\": {norm(w) for w in [\n",
    "        \"actor\", \"actors\", \"airman\", \"airmen\", \"uncle\", \"uncles\", \"boy\", \"boys\", \"groom\", \"grooms\", \"brother\", \"brothers\",\n",
    "        \"businessman\", \"businessmen\", \"chairman\", \"chairmen\", \"dude\", \"dudes\", \"dad\", \"dads\", \"daddy\", \"daddies\", \"son\", \"sons\",\n",
    "        \"father\", \"fathers\", \"male\", \"males\", \"guy\", \"guys\", \"gentleman\", \"gentlemen\", \"grandson\", \"grandsons\", \"he\", \"himself\",\n",
    "        \"him\", \"his\", \"husband\", \"husbands\", \"king\", \"kings\", \"lord\", \"lords\", \"sir\", \"man\", \"men\", \"mr.\", \"policeman\", \"prince\",\n",
    "        \"princes\", \"spokesman\", \"spokesmen\", \"actress\", \"actresses\", \"airwoman\", \"airwomen\", \"aunt\", \"aunts\", \"girl\", \"girls\",\n",
    "        \"bride\", \"brides\", \"sister\", \"sisters\", \"businesswoman\", \"businesswomen\", \"chairwoman\", \"chairwomen\", \"chick\", \"chicks\",\n",
    "        \"mom\", \"moms\", \"mommy\", \"mommies\", \"daughter\", \"daughters\", \"mother\", \"mothers\", \"female\", \"females\", \"gal\", \"gals\",\n",
    "        \"lady\", \"ladies\", \"granddaughter\", \"granddaughters\", \"she\", \"herself\", \"her\", \"wife\", \"wives\", \"queen\", \"queens\",\n",
    "        \"ma'am\", \"woman\", \"women\", \"mrs.\", \"ms.\", \"policewoman\", \"princess\", \"princesses\", \"spokeswoman\", \"spokeswomen\"\n",
    "    ]},\n",
    "    \"race\": {norm(w) for w in [\"black\", \"african\", \"africa\", \"caucasian\", \"white\", \"america\", \"europe\", \"asian\", \"asia\", \"china\"]},\n",
    "}\n",
    "\n",
    "SEP_TOKEN = \"<SEP>\"\n",
    "LEN_GUARD = 15\n",
    "\n",
    "def _validate(attr: str):\n",
    "    flat = {x for s in SUPPORTED.values() for x in s}\n",
    "    if attr not in flat:\n",
    "        raise ValueError(f\"'{attr}' is not a supported attribute. Choose one of: {', '.join(sorted(flat))}\")\n",
    "\n",
    "def make_prompt(word: str, attr: str, sentence: str) -> str:\n",
    "    _validate(attr)\n",
    "    return f\"{word}, {attr} {SEP_TOKEN} {sentence}\"\n",
    "\n",
    "def perturb_text(selected_word: str, target_attribute: str, sentence: str, *, max_new_tokens: int = 128, greedy: bool = False, top_p: float = 0.95, temperature: float = 0.8) -> str:\n",
    "    prompt = make_prompt(selected_word, target_attribute, sentence)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    gen_kwargs = {\"max_new_tokens\": max_new_tokens, \"do_sample\": not greedy}\n",
    "    if not greedy:\n",
    "        gen_kwargs.update({\"top_p\": top_p, \"temperature\": temperature})\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def _collect_pairs(sentence: str, axis2words: Dict[str, Set[str]]) -> List[Tuple[str, str]]:\n",
    "    tokens = [_norm(t) for t in re.findall(r\"\\b[\\w'-]+\\b\", sentence.lower())]\n",
    "    pairs: list[tuple[str, str]] = []\n",
    "    for axis, wordset in axis2words.items():\n",
    "        for tok in tokens:\n",
    "            if tok in wordset:\n",
    "                pairs.append((tok, axis))\n",
    "    return pairs\n",
    "\n",
    "def load_wordsets(base_dir: str | Path = \".\") -> Dict[str, Set[str]]:\n",
    "    base_dir = Path(base_dir)\n",
    "    axis2set: Dict[str, Set[str]] = {}\n",
    "    for axis in (\"gender\", \"race\"):\n",
    "        fname = base_dir / f\"{axis}_words.txt\"\n",
    "        if not fname.exists():\n",
    "            raise FileNotFoundError(f\"Missing file: {fname}\")\n",
    "        axis2set[axis] = {_norm(w) for w in fname.read_text(encoding=\"utf-8\").splitlines() if w.strip()}\n",
    "    return axis2set\n",
    "\n",
    "def detect_bias_words(sentence: str, *, base_dir: str | Path = \".\") -> List[Tuple[str, str]]:\n",
    "    tokens = [_norm(tok) for tok in re.findall(r\"\\b[\\w'-]+\\b\", sentence.lower())]\n",
    "    matches: List[Tuple[str, str]] = []\n",
    "    for axis, wordset in PRIORITY_WORDS.items():\n",
    "        for tok in tokens:\n",
    "            if tok in wordset:\n",
    "                matches.append((tok, axis))\n",
    "    if matches:\n",
    "        return matches\n",
    "    fallback_sets = load_wordsets(base_dir)\n",
    "    for axis, wordset in fallback_sets.items():\n",
    "        for tok in tokens:\n",
    "            if tok in wordset:\n",
    "                matches.append((tok, axis))\n",
    "    return matches\n",
    "\n",
    "def unique_bias_words(sentence: str, *, base_dir: str | Path = \".\") -> List[Tuple[str, str]]:\n",
    "    seen = {}\n",
    "    for pair in detect_bias_words(sentence, base_dir=base_dir):\n",
    "        if pair not in seen:\n",
    "            seen[pair] = None\n",
    "    return list(seen)\n",
    "\n",
    "def _attempt_perturb(sentence: str, word: str, axis: str, *, greedy: bool) -> Tuple[str, bool]:\n",
    "    subcategory = random.choice(list(SUPPORTED[axis]))\n",
    "    new_sentence = perturb_text(word, subcategory, sentence, greedy=greedy)\n",
    "    length_ok = (len(sentence) - len(new_sentence)) <= LEN_GUARD\n",
    "    return new_sentence, length_ok\n",
    "\n",
    "def random_perturbation(sentence: str, *, base_dir: str | Path = \".\", greedy: bool = True) -> Tuple[bool, str, str | None, str | None, str | None]:\n",
    "    priority_pairs = unique_bias_words_from_sets(sentence, PRIORITY_WORDS)\n",
    "    changed, new_sent, word, axis, cat = _try_pairs(sentence, priority_pairs, greedy)\n",
    "    if changed:\n",
    "        return changed, new_sent, word, axis, cat\n",
    "    fallback_sets = load_wordsets(base_dir)\n",
    "    tried_tokens = {w for w, _ in priority_pairs}\n",
    "    for axis in fallback_sets:\n",
    "        fallback_sets[axis] -= tried_tokens\n",
    "    fallback_pairs = unique_bias_words_from_sets(sentence, fallback_sets)\n",
    "    return _try_pairs(sentence, fallback_pairs, greedy)\n",
    "\n",
    "def unique_bias_words_from_sets(sentence: str, axis2set: Dict[str, Set[str]]) -> list[tuple[str, str]]:\n",
    "    seen = set()\n",
    "    ordered: list[tuple[str, str]] = []\n",
    "    for pair in _collect_pairs(sentence, axis2set):\n",
    "        if pair not in seen:\n",
    "            seen.add(pair)\n",
    "            ordered.append(pair)\n",
    "    return ordered\n",
    "\n",
    "def _try_pairs(sentence: str, pairs: list[tuple[str, str]], greedy: bool) -> tuple[bool, str, str | None, str | None, str | None]:\n",
    "    if not pairs:\n",
    "        return False, sentence, None, None, None\n",
    "    axis_order = [\"gender\", \"race\"]\n",
    "    random.shuffle(axis_order)\n",
    "    for axis_choice in axis_order:\n",
    "        words = [w for w, ax in pairs if ax == axis_choice]\n",
    "        random.shuffle(words)\n",
    "        while words:\n",
    "            chosen_word = words.pop()\n",
    "            first_draft, length_ok = _attempt_perturb(sentence, chosen_word, axis_choice, greedy=greedy)\n",
    "            if SEP_TOKEN in first_draft:\n",
    "                continue\n",
    "            if not length_ok:\n",
    "                continue\n",
    "            subcategory = random.choice(list(SUPPORTED[axis_choice]))\n",
    "            final_draft = perturb_text(chosen_word, subcategory, sentence, greedy=greedy)\n",
    "            if SEP_TOKEN in final_draft:\n",
    "                continue\n",
    "            if (len(sentence) - len(final_draft)) > LEN_GUARD:\n",
    "                continue\n",
    "            changed = _simplify(final_draft) != _simplify(sentence)\n",
    "            if changed:\n",
    "                return changed, final_draft, chosen_word, axis_choice, subcategory\n",
    "    return False, sentence, None, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TEST_SENTENCE = \"The climbdown means the savings will now be delayed or lost entirely...\"\n",
    "    changed, new_sent, word, axis, cat = random_perturbation(TEST_SENTENCE)\n",
    "    print(new_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751814999540
    }
   },
   "outputs": [],
   "source": [
    "#corpus perturbing\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import argparse, random, sys, torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "INPUT_FILE = Path(\"chunks_sentences.txt\")\n",
    "OUTPUT_FILE = Path(\"chunks_sentences_perturbed.txt\")\n",
    "METRICS_FILE = Path(\"perturbation_metrics.txt\")\n",
    "DEFAULT_BATCH_SIZE = 256\n",
    "MICRO_BATCH_SIZE = 8\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\").half().eval()\n",
    "else:\n",
    "    model = model.eval()\n",
    "\n",
    "if getattr(model, \"generation_config\", None) and getattr(model.generation_config, \"early_stopping\", None) is not False:\n",
    "    model.generation_config.early_stopping = False\n",
    "\n",
    "try:\n",
    "    autocast_ctx = lambda: torch.amp.autocast(device_type=\"cuda\")\n",
    "except AttributeError:\n",
    "    autocast_ctx = torch.cuda.amp.autocast if torch.cuda.is_available() else nullcontext\n",
    "\n",
    "def process_batch(lines, fout, *, stats, micro_bs=MICRO_BATCH_SIZE):\n",
    "    device = model.device\n",
    "    prompts, meta, valid = [], [], []\n",
    "    for sent in lines:\n",
    "        pairs = unique_bias_words(sent)\n",
    "        if not pairs:\n",
    "            prompts.append(None)\n",
    "            meta.append((sent, None, None))\n",
    "            continue\n",
    "        word, axis = random.choice(pairs)\n",
    "        subcat = random.choice(list(SUPPORTED[axis]))\n",
    "        prompts.append(make_prompt(word, subcat, sent))\n",
    "        meta.append((sent, axis, subcat))\n",
    "        valid.append(len(prompts) - 1)\n",
    "\n",
    "    gen_map = {}\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(valid), micro_bs):\n",
    "            idx_slice = valid[i: i + micro_bs]\n",
    "            try:\n",
    "                batch_prompts = [prompts[j] for j in idx_slice]\n",
    "                enc = tokenizer(batch_prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "                with autocast_ctx():\n",
    "                    outs = model.generate(\n",
    "                        **enc,\n",
    "                        max_new_tokens=128,\n",
    "                        do_sample=False,\n",
    "                        use_cache=False,\n",
    "                        num_beams=1,\n",
    "                    )\n",
    "                decoded = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "                for k, j in enumerate(idx_slice):\n",
    "                    gen_map[j] = decoded[k]\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower() and micro_bs > 1:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    process_batch(\n",
    "                        [lines[j] for j in idx_slice],\n",
    "                        fout, stats=stats, micro_bs=micro_bs // 2,\n",
    "                    )\n",
    "                else:\n",
    "                    raise\n",
    "            finally:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    for i, (orig, axis, subcat) in enumerate(meta):\n",
    "        new_sent = gen_map.get(i, orig)\n",
    "        changed = _simplify(new_sent) != _simplify(orig)\n",
    "        fout.write((new_sent if changed else orig) + \"\\n\")\n",
    "        stats[\"n_total\"] += 1\n",
    "        if changed:\n",
    "            stats[\"n_changed\"] += 1\n",
    "            if axis:\n",
    "                stats[\"axis_counts\"][axis] += 1\n",
    "            if axis and subcat:\n",
    "                stats[\"subcat_counts\"][(axis, subcat)] += 1\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=DEFAULT_BATCH_SIZE)\n",
    "    parser.add_argument(\"--start-line\", type=int, default=1)\n",
    "    parser.add_argument(\"--end-line\", type=int, default=1054548)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if not INPUT_FILE.exists():\n",
    "        sys.exit(f\"Cannot find {INPUT_FILE}\")\n",
    "\n",
    "    with INPUT_FILE.open(encoding=\"utf-8\") as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "\n",
    "    effective_end = args.end_line if args.end_line is not None else total_lines\n",
    "    if args.start_line < 1 or effective_end < args.start_line or effective_end > total_lines:\n",
    "        sys.exit(\"Invalid range\")\n",
    "    selected_total = effective_end - args.start_line + 1\n",
    "\n",
    "    if tqdm is not None:\n",
    "        pbar = tqdm(total=selected_total, unit=\"line\", desc=\"Perturbing\")\n",
    "        update_pbar = pbar.update\n",
    "    else:\n",
    "        update_pbar = lambda n=1: None\n",
    "\n",
    "    stats = dict(n_total=0, n_changed=0, axis_counts=Counter(), subcat_counts=Counter())\n",
    "\n",
    "    with INPUT_FILE.open(encoding=\"utf-8\") as fin, OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        inside_batch = []\n",
    "        for lineno, raw in enumerate(fin, start=1):\n",
    "            stripped = raw.rstrip(\"\\n\")\n",
    "            if args.start_line <= lineno <= effective_end:\n",
    "                inside_batch.append(stripped)\n",
    "                update_pbar(1)\n",
    "                if len(inside_batch) >= args.batch_size:\n",
    "                    process_batch(inside_batch, fout, stats=stats)\n",
    "                    inside_batch.clear()\n",
    "            else:\n",
    "                fout.write(\"\\n\")\n",
    "        if inside_batch:\n",
    "            process_batch(inside_batch, fout, stats=stats)\n",
    "\n",
    "    if tqdm is not None:\n",
    "        pbar.close()\n",
    "\n",
    "    n_total, n_changed = stats[\"n_total\"], stats[\"n_changed\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
