# Evaluation pipelines for performance and bias
The two main files are _performance_eval.py_, which evaluates performace, and _bias_eval.py_, which evaluates bias (CrowS-Pairs and StereoSet), for any list of eligible Hugging Face LMs.

To run _bias_eval.py_, please install Language Model Evaluation Harness: 
```bash
pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git
```

